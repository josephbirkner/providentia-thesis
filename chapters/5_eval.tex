% !TeX spellcheck = en_US

\chapter{Evaluation}
\label{ch:evaluation}

\section{Qualitative Results}
\label{sec:qualres}

To acquire a first impression of the strengths and weaknesses of our fully implemented HD-map-assisted monocular 3D object detector, some sample detection frames are presented here which showcase particular situations.

\begin{figure}[htb]
    \includegraphics[width=0.499\linewidth]{
        figures/selection/1646667323.129810658.s110.camera.basler.south2.8mm}
    \includegraphics[width=0.499\linewidth]{
        figures/selection/1646667339.058779966.s110.camera.basler.south1.8mm} \\
    \includegraphics[width=0.499\linewidth]{
        figures/selection/1651673054.274505919.s110.camera.basler.south2.8mm}
    \includegraphics[width=0.499\linewidth]{
        figures/selection/1651673056.492970825.s110.camera.basler.south1.8mm}
    \caption{Hand-picked frames to showcase the qualitative performance of our implemented monocular 3D object detector at daytime.}
    \label{fig:qualitative-results-day}
\end{figure}

First off, Figure~\ref{fig:qualitative-results-day} shows representative frames with detection annotations, where the implemented system performs quite well.
The frames in the left column are from the South-2 camera, which has a greater detection range and wider field-of-view, as it is angled more towards towards the horizon.
The two frames for this camera show, that the implemented system can correctly detect the 3D shapes of many overlapping vehicles, which are oriented opposed or perpendicular to each other.
The top-left frame showcases the good performance of the screen-space \textit{SORT} tracker in particular, which tracks \texttt{CAR 896} throughout its whole left turn.
The bottom-left image shows the usefulness of the \textit{DBSCAN}-based bottom contour filter in the case of \texttt{BUS 375}.
The detection of this bus is split into two parts by the pole of the overhead gantry bridge.
This introduces a lot of noise into this detection's bottom contour, as highlighted in yellow.
However, the detected 3D bounding box is unaffected by this noise.
This frame also showcases the performance of our Vulnerable Road User (VRU) detection.
Both present pedestrians and bicycles in the image are correctly localized.

The frames in the right column are from the South-1 camera, which has a shorter detection range and narrower field-of-view, as it is angled more towards towards the ground.
Both images in the right column again show generally good vehicle tracking and orientation estimation capabilities.

\begin{figure}[htb]
    \includegraphics[width=0.499\linewidth]{
        figures/selection/1646667395.641065639.s110.camera.basler.south1.8mm}
    \includegraphics[width=0.499\linewidth]{
        figures/selection/1651673050.162472285.s110.camera.basler.south1.8mm}
    \caption{Selected frames to showcase particular problems of our implemented monocular 3D object detector.}
    \label{fig:qualitative-results-bad}
\end{figure}

Figure~\ref{fig:qualitative-results-bad} highlights particular weaknesses of the implemented monocular detector.
The image on the left highlights three problems.
First, the length estimation of \texttt{TRUCK 42} is bad, most likely due to over-eager bottom-countour filtering.
Second, only one of the two pedestrians is detected \textemdash \texttt{PEDESTRIAN 4} is standing next to another one.
Third, the system detects the rider of a bicycle as a separate entity, as in case of \texttt{BICYCLE 9} and \texttt{PEDESTRIAN 81}.
This points to a need for more fine-tuned non-maximum suppression (NMS) of 3D detections.
The image on the right shows a single problem: Our detector (more specifically the used \textit{Yolov7} instance segmentation model) frequently has problems with detecting the instance masks of large objects near the camera.
In this case, both the big white truck in front of the camera, and the bus which is a bit further away, are not detected.

\begin{figure}[htb]
    \includegraphics[width=0.499\linewidth]{
        figures/selection/1653330059.588901912.s110.camera.basler.south2.8mm}
    \includegraphics[width=0.499\linewidth]{
        figures/selection/1653330064.207615030.s110.camera.basler.south1.8mm}
    \caption{A selection of frames to showcase the qualitative performance of our implemented monocular 3D object detector at night.}
    \label{fig:qualitative-results-night}
\end{figure}

Finally, Figure~\ref{fig:qualitative-results-night} displays how the system performs at night.
Naturally, the detection performance of a system which is based on an RGB camera which operates in the human visible light spectrum will be worse at night, due to the reduced visibility.
However, the used \textit{Yolov7} instance segmentation model still works to some degree at night.
The predicted instance masks are generally noisier, and there more false positives (such as \texttt{PEDESTRIAN 13}) and false negatives (such as the truck on the top-left in the right image).

% ----------------------------------------------------

\section{Quantitative Evaluation Strategy}
\label{sec:quant}

For an objective understanding of the performance of our \textit{Mono3D} solution, we performed a thorough ablative quantitative evaluation.
For this purpose, we use the \textit{Providentia A9R1} dataset, as previously mentioned in the Introduction (see Chapter~\ref{sec:a9dataset}).
As stated, the dataset provides 3D lidar label annotations for two camera perspectives onto the urban \textit{S110} road intersections.
For each camera, four scenes of varying length (between 300 and 1200 frames) are available.
The combination of two camera perspectives and four scenes yields eight frame sequences on which we evaluate our detector.
For each frame sequence, the detector is run in different configurations to evaluate the effect of various design choices on the final performance.
The detector is configured along four major components:

\begin{enumerate}
    \item \textbf{Instance Segmentation}: In the first detector stage,\ We can employ the \textit{Yolact}~\cite{liu2021yolactedge} instance segmentation model (running on 550x550 input frames), or the \textit{YoloV7}~\cite{wang2022yolov7} detector in 640x640, 1280x180, or 1920x1920 px resolution mode.\ We respectively designate these modes as $I^{550}_\text{YOL}$, $I^{640}_\text{Yv7}$, $I^{1280}_\text{Yv7}$, and $I^{1920}_\text{Yv7}$.
    \item \textbf{Tracking}: This component concerns the usage of \textit{SORT tracking}~\cite{bewley2016simple} in our detector.\ We can apply tracking in $2D$ screen-space to assist the LSF algorithm, or in $3D$ space to stabilize vehicle position estimates, or both in $2D$ and $3D$, or not at all.\ We respectively designate these modes as $T_{2D}$, $T_{3D}$, $T^{2D}_{3D}$, and $T_0$.
    \item \textbf{HD-Map Usage}: In this aspect, we consider whether to use heading information from the HD-map as an early input to the LSF algorithm, as a late single-position lookup correction, or not at all.\ These modes are designated as $M_\text{LSF}$, $M_1$ and $M_0$.
    \item \textbf{Filtering}: Finally, we can switch two crucial filters on or off:\ The \textit{DBSCAN}~\cite{schubert2017dbscan} bottom contour point filter, and the output vehicle size filter.\ These modes will be symbolized as $F_\text{Cont}$, $F_\text{Size}$, $F_\text{Cont}^\text{Size}$, or $F_0$ (if no filter is used).
\end{enumerate}

In summary, each detector configuration is some combination of $I$, $T$, $M$ and $F$.\ For example, $\left[I^{640}_\text{Yv7}T^{2D}_{3D}M_\text{LSF}F_\text{Size}\right]$ would be the detector running with 640x640 px \textit{Yolov7} instance segmentation, both 2D and 3D tracking, L-Shape-Fitting Map Input, and vehicle size filtering (but no bottom-contour filtering).
The combinatorial expansion yields 192 possible detector configurations, which are executed on each of the 8 LIDAR-labeled camera frame sequences.
This results in 1536 prediction sequences.

Furthermore, we evaluate the impact which emerges from the sensor delay between the camera and the LIDAR sensor frames.
This delay is roughly 149ms on average.
This synchronization error is important, because we evaluate the 3D camera detections on labeled LIDAR sensor measurements.
These LIDAR labels will contain an inherent offset towards the camera detections due to the synchronization error.
We check whether the error can be reduced by estimating the spatial velocity of the LIDAR detections using \textit{SORT}, and then correcting the label position based on the velocity and the known synchronization error time delta.

We evaluate both the time-shifted and the original LIDAR labels with each prediction sequence.
We call these two label modes $L_0$ (original) and $L_{\uparrow}$ (shifted).
The label mode is appended to the detector configuration combination.
For each of the 3072 sequence-detector-label combinations and each road suer class, we calculate the following metrics:

\begin{enumerate}
    \item The average orientation error \textbf{(AOE)} ($=\text{TP}_0$).
    \item The average translation error \textbf{(ATE)} ($=\text{TP}_1$.
    \item The average length error \textbf{(ALE)} ($=\text{TP}_2$).
    \item The average width error \textbf{(AWE)} ($=\text{TP}_3$).
    \item The average height error \textbf{(AHE)} ($=\text{TP}_4$).
    \item The average precision at 10\% overlap \textbf{(AP@10)}.
    \item The average 3D intersection-over-union ($\mathbf{IoU}_{3D}$)
\end{enumerate}

In the following, the values for these metrics are presented in relation to particular detector and labeling configurations.
To judge the overall performance of a configuration, we aggregate these metrics into a single \enquote{Providentia Detection Score}, similar to the \textit{nuScenes Detection Score} (see Section~\ref{sec:evalmetrics}):

\[
    \text{Score} = \frac{1}{10} \left[5 * \text{mAP} + \sum^5_{k=1}(1-\text{min}(1, \text{mTP}_k))\right]
\]

The score is calculated for each configuration, by averaging the metrics of the configuration across all perspectives of all dataset scenes.
A chart of all calculated Scores is shown in Figure~\ref{fig:all-scores}.

% ----------------------------------------------------

\section{Overall Quantitative Results}
\label{sec:baseline}

\begin{figure}[htb]
    \input{charts/overall}
    \caption{Overview for the Score values of all possible 384 detector configuration-label combinations, with arrows pointing out the best, baseline, and worst combination.}
    \label{fig:all-scores}
\end{figure}

In order to establish the veracity of our score metric, we plotted the score values for all possible 384 detector configuration-label combinations into a single chart.
This plot is shown in Figure~\ref{fig:all-scores}.
In this plot, the Scores of the worst-performing- ($\left[I^{550}_\text{YOL}T^{2D}_{3D}M_1F_0L_{\uparrow}\right]$), baseline- ( $\left[I^{550}_\text{YOL}T_0M_0F_0L_0\right]$), and best-performing ($\left[I^{1920}_\text{Yv7}T_{2D}M_\text{LSF}F_\text{Cont}^\text{Size}L_{\uparrow}\right]$) model-configurations are also highlighted.
The baseline model works with pure L-Shape-Fitting that does not use any HD-map, tracking, or bottom-contour/size filtering augmentations.
Table~\ref{tbl:baseline} shows the performance of this model in greater detail.
Of particular interest is the fairly large AOE of $45.44\degree$.
This means, that the predicted orientation of bounding boxes in this model is  completely disconnected from the ground truth.
Picking a random orientation would yield the same result.

\begin{table*}[htbp]
    \input{charts/baseline}
    \caption{Baseline model results.}
    \label{tbl:baseline}
\end{table*}

This stands in contrast to the performance metrics for the best-performing model, which are shown in Table~\ref{tbl:best}.
In the following sections, we will discuss in detail how specific model configurations improve (or worsen) specific metrics for different road user classes.

\begin{table*}[htbp]
    \input{charts/best}
    \caption{Best model results, with improvements towards the baseline highlighted in green for each metric.}
    \label{tbl:best}
\end{table*}

% ----------------------------------------------------

\section{Impact of Late HD Map Lookup}
\label{sec:impactlatemap}

In this section, we present the evaluation of the \enquote{Late HD Map Lookup} technique, a simpler alternative to the \enquote{early} HD map lookup.
As mentioned in Section~\ref{sec:hdmaplate}, this approach has a number of drawbacks which are expected to negatively impact the system's performance.
The purpose of this evaluation is to provide a clear understanding of the limitations and consequences of using the late lookup approach.
This was the first approach we tried to improve upon the baseline detector.
The metrics of a Yolact-based detector which simply adds late HD-map-lookup-based heading correction are presented in Table~\ref{tbl:late-lookup-results}.

\begin{table*}[htbp]
    \input{charts/late-lookup}
    \caption{Results for the simplest possible late map lookup detector, with differences towards the baseline highlighted in red and green.}
    \label{tbl:late-lookup-results}
\end{table*}

One of the main issues with the late HD map lookup is that it takes place after the L-Shape-Fitting has already determined a heading value.
This means that the heading information from the HD map cannot be used to guide or correct the LSF-based size , leading to inaccuracies in the width and length estimation.
This is also showing in the results, as none of the size or translation metrics are significantly improved.

Another significant drawback of the late HD map lookup is that it discards a lot of the positional information from the ground-truth sensor input.
By querying the HD map only for heading options at the predicted vehicle position, the system loses the ability to take advantage of the more accurate and granular bottom contour data from the sensor inputs.
This loss of information is reflected in the barely improved orientation error.

% ----------------------------------------------------

\section{Impact of YOLOv7 and Image Resolution}
\label{sec:impactyolov7}

The comparison of the average performance of detectors using different instance segmentation models highlights some interesting points, as shown in Table~\ref{tbl:resolution-results}.
Comparing the performance of the \textit{Yolact-Edge} model with the Yolov7 models at different resolutions, we can see that the Yolov7 models generally outperform the \textit{Yolact-Edge} model.
The \textit{Yolov7} ($640x640$) model shows an average score improvement of $1.35\%$ compared to the \textit{Yolact-Edge} model, with an increase in mean AP by $3.05\%$.
Similarly, \textit{Yolov7} (1280x1280) has a score improvement of $1.84\%$ compared to \textit{Yolov7} (640x640), and a higher mean AP of $5.02\%$.

It is important to note that \textit{Yolov7} (1920x1920) does not outperform \textit{Yolov7} (1280x1280) on average, as it has a slightly lower overall score by $0.62\%$.
However, the pedestrian detection performance is greatly improved by the \textit{Yolov7} (1920x1920) model, as its AP for pedestrian detection is at $20.38\%$, compared to $13.01\%$ in the \textit{Yolov7} (640x640) model and only $0.63\%$ in the \textit{Yolact-Edge} model.

The results indicate that higher-resolution input may not always result in better overall performance, but can significantly improve the detection of specific object classes like pedestrians.
This suggests that for certain applications or scenarios, focusing on higher resolutions could be beneficial.

In summary, while \textit{Yolov7} (1920x1920) does not show an overall better performance than \textit{Yolov7} (1280x1280), its improvement in pedestrian detection is noteworthy.
The \textit{Yolov7} models outperform the \textit{Yolact-Edge} model, with the \textit{Yolov7} (1280x1280) showing the best average metrics among the compared models.

\begin{table*}[htbp]
    \input{charts/resolution}
    \caption{Average results for different instance segmentation models, with differences in the resulting metrics shown towards the previous model's average values.}
    \label{tbl:resolution-results}
\end{table*}

% ----------------------------------------------------

\newpage

\section{Impact of Filters}
\label{sec:impactcontourfiltering}

Furthermore, we evaluate the impact of \textit{DBSCAN}-based bottom-contour filtering and size filtering on the detector performance.
The results presented in Table~\ref{tbl:filter-results} show the impact of different filter types on the 3D object detection task.
Three different filter types are considered for comparison: no filters, contour filters, and size filters.
For each filter type, the best-performing model is selected and compared against the overall best model, which uses both filters.
The following conclusions can be drawn for these configurations:

\textbf{No Filters:} The best model without any filters yields an overall score of $31.27\%$, which is the lowest among the three variations.
All the metrics exhibit worse performance compared to the best model.
This indicates that filters can be essential for improving the model's performance.

\textbf{Contour Filters:} When contour filters are applied, the overall score increases to $36.57\%$.
Although there is an improvement in the mean values for AOE and ALE, the other metrics such as ATE, AWE, and IoU show a slight decrease compared to the best model.
Nonetheless, the contour filters help to enhance the overall performance of the model.

\textbf{Size Filters:} Applying size filters results in the highest overall score of $39.97\%$, which is only $0.20\%$ lower than the best model.
The size filter leads to improvements in AOE, AWE, Precision, and AP, while causing a slight decrease in ATE, ALE, AHE, and IoU.
Despite these mixed results, the size filters demonstrate the most promising impact on the model's performance.

In summary, using filters in the 3D object detection task can substantially improve the model's performance.
Among the different filter types, size filters show the most significant positive impact on the overall score, while bottom contour filters also provide a noticeable improvement.

\begin{figure}[htb]
    \includegraphics[width=0.499\linewidth]{
        figures/selection/1651673054.274505919.s110.camera.basler.south2.8mm.nofilter}
    \includegraphics[width=0.499\linewidth]{
        figures/selection/1651673054.274505919.s110.camera.basler.south2.8mm}
    \caption{Comparison of the same frame from our best model ($\left[I^{1920}_\text{Yv7}T_{2D}M_\text{LSF}F_\text{Cont}^\text{Size}\right]$) (right), vs. the same model except with disabled filtering (left). }
    \label{fig:compare-without-filter}
\end{figure}

Figure~\ref{fig:compare-without-filter} further illustrates the effects of the filters on the quality of the detection result.

\begin{table*}[htbp]
    \input{charts/filters}
    \caption{Average results for models which use \textit{DBSCAN}-based bottom contour filtering and/or size filtering, compared with those models which do not use any filters. It is apparent, that the filtering mostly has a positive effect across all metrics, and the filters also synergize with each other.}
    \label{tbl:filter-results}
\end{table*}

\newpage

% ----------------------------------------------------

\section{Impact of L-Shape-Fitting Augmentations}
\label{sec:lsf-aug-impact}

In this section, we will discuss the impact of different LSF augmentations on the detector's performance.
Two augmentations are tested: HD-Map Input and Screen-Space Tracking.
We will analyze the results of these augmentations and compare them with the best performance achieved without LSF Augments.
As an initial overview, Figure~\ref{fig:lsf-augmentations-overview} provides a rough indication for the performance distribution of different augmentation configurations.

\begin{figure}[htb]
    \input{charts/lsf-augmentations}
    \caption{Overview for model performances when model uses augmentations.}
    \label{fig:lsf-augmentations-overview}
\end{figure}

\subsection{Impact of HD-Map Augmentation}
\label{subsec:impactmap}

The best-performing model which uses only the HD-Map LSF augmentation ($\left[I^{1920}\text{Yv7}T_0M\text{LSF}F_\text{Cont}^\text{Size}L_{\uparrow}\right]$), reaches a Score of $40.11\%$, which is only $0.06\%$ lower than the best model, which uses both LSF Augments.
The mean Average Orientation Error (AOE) was significantly reduced to $13.35\degree$, which beats the overall best model by $0.45\degree$.
However, the Intersection over Union (IoU) metric is worse by $0.31\%$, and the Recall is worse by $0.19\%$.
Overall, the HD-Map Input LSF augmentation demonstrates a very positive impact on the detector's performance.

\subsection{Impact of Screen-Space Tracking Augmentation}
\label{subsec:impactlsf}

The best-performing model which uses only the Screen-Space Tracking LSF augmentation ($\left[I^{1920}\text{Yv7}T{2D}M_1F_\text{Cont}^\text{Size}L_{\uparrow}\right]$) reaches a Score of $34.95\%$, which is $5.22\%$ lower than the best model.
Compared to the best un-augmented model, the mean AOE remains almost unchanged at $42.94\degree$.
Interestingly, the model beats the best model barely on Precision, Recall, and AP.
Overall, the sole Screen-Space Tracking LSF augmentation shows mixed results, with some improvements in Precision and Recall but a lower overall Score.
We can conclude, that Tracking alone is insufficient as a bias for vehicle orientation estimation.
However, the final best model still uses this augmentation in conjunction with the HD-map, and thereby manages to reach the highest score.

\begin{table*}[htbp]
    \input{charts/lsf}
    \caption{Best result for model which uses raw L-Shape-Fitting (LSF), compared with best results for models which use just one of the Screen-Space Tracking or HD-Map Lookup Augmentations.}
    \label{tbl:lsf-results}
\end{table*}

\newpage

% ----------------------------------------------------

\section{Impact of 3D Tracking}
\label{sec:impacttracking}

\begin{table*}[htbp]
    \input{charts/track-3d}
    \caption{Best result for model which uses 3D Tracking, compared with the best model.}
    \label{tbl:track-3d-results}
\end{table*}

\section{Impact of LIDAR Label Shifting}
\label{sec:impactlidar}

\begin{table*}[htbp]
    \input{charts/label-shift}
    \caption{Average results for evaluations with time-shifted labels compared to the original unsynchronized LIDAR labels.}
    \label{tbl:label-shift-results}
\end{table*}

\newpage

% ----------------------------------------------------

\section{Impact of Night-time Conditions}
\label{sec:weather}

\begin{table*}[htbp]
    \input{charts/night}
    \caption{Average results for evaluations on the night-time scene of the A9 dataset with \textit{Yolact-Edge} and \textit{Yolov7} models.}
    \label{tbl:nighttime-results}
\end{table*}

\newpage

% ----------------------------------------------------

\section{Performance}
\label{sec:performance}

Small table of average FPS with different models.

% ----------------------------------------------------

\section{Summary}
\label{sec:summary}

Summarize evaluation results

Do some meta-evaluation (dataset problems, VRU detection, height-position estimation).

% ----------------------------------------------------
